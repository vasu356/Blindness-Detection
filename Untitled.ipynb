{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655ef72b-0736-4529-9ba0-34c149b9348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a47f893-5860-4e9d-b41d-89a697383c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train_images'\n",
    "TEST_DIR = 'images/test_images'\n",
    "TRAIN_CSV = 'labels/train.csv'\n",
    "TEST_CSV = 'labels/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65e8349-a340-4e59-a25b-5fbcb8d6138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = pd.read_csv(TRAIN_CSV)\n",
    "test_labels_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5d56f6-8d8a-434c-aa25-66c2c4b94ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir, labels_df):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for index, row in labels_df.iterrows():\n",
    "        image_name = row['id_code']\n",
    "        label = row['diagnosis']\n",
    "        image_paths.append(os.path.join(dir, image_name + '.png'))\n",
    "        labels.append(label)\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2056bd50-68b8-49e5-9ca2-2ec385658426",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = createdataframe(TRAIN_DIR, train_labels_df)\n",
    "test_images, test_labels = createdataframe(TEST_DIR, test_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce857de9-1c0f-4bda-a97e-5abb276bbbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2930/2930 [08:19<00:00,  5.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 366/366 [00:56<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_images(image_paths, target_size=(224, 224)):\n",
    "    images = []\n",
    "    for image_path in tqdm(image_paths):\n",
    "        img = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load and preprocess train and test images\n",
    "x_train = preprocess_images(train_images)\n",
    "x_test = preprocess_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d31916d-fd9d-4ec2-9f83-fcc88cce0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=5)\n",
    "y_test = to_categorical(y_test, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8381e783-14c7-4f4e-bdb4-1f816748f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Adding Convolutional Layers\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7623a988-1b5e-4056-8bdf-59d656dbb348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "23/23 [==============================] - 1242s 52s/step - loss: 2.7489 - accuracy: 0.4304 - val_loss: 1.2266 - val_accuracy: 0.5437\n",
      "Epoch 2/15\n",
      "23/23 [==============================] - 1178s 51s/step - loss: 1.0497 - accuracy: 0.6232 - val_loss: 0.9086 - val_accuracy: 0.7022\n",
      "Epoch 3/15\n",
      "23/23 [==============================] - 1108s 48s/step - loss: 0.8793 - accuracy: 0.6870 - val_loss: 0.7790 - val_accuracy: 0.7186\n",
      "Epoch 4/15\n",
      "23/23 [==============================] - 1102s 48s/step - loss: 0.8388 - accuracy: 0.6949 - val_loss: 0.7527 - val_accuracy: 0.7240\n",
      "Epoch 5/15\n",
      "23/23 [==============================] - 1091s 47s/step - loss: 0.8176 - accuracy: 0.7092 - val_loss: 0.7493 - val_accuracy: 0.7432\n",
      "Epoch 6/15\n",
      "23/23 [==============================] - 1109s 48s/step - loss: 0.7998 - accuracy: 0.7068 - val_loss: 0.7487 - val_accuracy: 0.7432\n",
      "Epoch 7/15\n",
      "23/23 [==============================] - 1125s 49s/step - loss: 0.7902 - accuracy: 0.7171 - val_loss: 0.7253 - val_accuracy: 0.7377\n",
      "Epoch 8/15\n",
      "23/23 [==============================] - 1114s 49s/step - loss: 0.7738 - accuracy: 0.7164 - val_loss: 0.7221 - val_accuracy: 0.7404\n",
      "Epoch 9/15\n",
      "23/23 [==============================] - 1110s 48s/step - loss: 0.7531 - accuracy: 0.7273 - val_loss: 0.7102 - val_accuracy: 0.7486\n",
      "Epoch 10/15\n",
      "23/23 [==============================] - 1107s 48s/step - loss: 0.7445 - accuracy: 0.7273 - val_loss: 0.7261 - val_accuracy: 0.7404\n",
      "Epoch 11/15\n",
      "23/23 [==============================] - 1120s 48s/step - loss: 0.7339 - accuracy: 0.7379 - val_loss: 0.6991 - val_accuracy: 0.7377\n",
      "Epoch 12/15\n",
      "23/23 [==============================] - 1077s 47s/step - loss: 0.7165 - accuracy: 0.7341 - val_loss: 0.6999 - val_accuracy: 0.7377\n",
      "Epoch 13/15\n",
      "23/23 [==============================] - 1099s 48s/step - loss: 0.7027 - accuracy: 0.7423 - val_loss: 0.6909 - val_accuracy: 0.7459\n",
      "Epoch 14/15\n",
      "23/23 [==============================] - 1087s 47s/step - loss: 0.6792 - accuracy: 0.7488 - val_loss: 0.6993 - val_accuracy: 0.7459\n",
      "Epoch 15/15\n",
      "23/23 [==============================] - 1074s 47s/step - loss: 0.6538 - accuracy: 0.7635 - val_loss: 0.6957 - val_accuracy: 0.7322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23522781f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=15, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4853e34f-aa44-4284-a98e-711d30807648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Blindness_Detection.h5\")\n",
    "np.save('label_encoding.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d14ba93d-70e1-459d-bbea-67d40fdb5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f859e06-77bd-49d3-9e91-9573f110d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"Blindness_Detection.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model weights\n",
    "model.save_weights(\"Blindness_Detection.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d0028fd-5e23-4e05-8a5b-718440ba5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Load the model architecture from JSON\n",
    "with open(\"Blindness_Detection.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the model weights into the loaded model\n",
    "loaded_model.load_weights(\"Blindness_Detection.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e068220-928c-443f-93fc-cab414e12d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Blindness_Detection.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd23503f-d2c6-47f2-a237-cd8324a7e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original image is of label 4\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Label: [[0.03957573 0.2795418  0.41074705 0.12372563 0.14640985]]\n",
      "Predicted Label: 2\n",
      "The original image is of label 3\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "Predicted Label: [[0.00318081 0.01752213 0.79349923 0.10183981 0.08395811]]\n",
      "Predicted Label: 2\n",
      "The original image is of label 2\n",
      "1/1 [==============================] - 0s 497ms/step\n",
      "Predicted Label: [[9.7147567e-05 1.2930850e-02 7.5904047e-01 3.2927945e-02 1.9500355e-01]]\n",
      "Predicted Label: 2\n",
      "The original image is of label 1\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "Predicted Label: [[0.11991165 0.39930627 0.27562588 0.06825943 0.13689674]]\n",
      "Predicted Label: 1\n",
      "The original image is of label 0\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "Predicted Label: [[0.5047441  0.02903391 0.33556587 0.01541239 0.1152438 ]]\n",
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def ef(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.reshape(1, 224, 224, 1)\n",
    "    return img / 255.0\n",
    "\n",
    "image_path4 = 'images/test_images/e7d2c2c3b30f.png'\n",
    "image_path3 = 'images/test_images/e93394175a19.png'\n",
    "image_path2 = 'images/test_images/f61bf44c677c.png'\n",
    "image_path1 = 'images/test_images/fb6b8200b7f8.png'\n",
    "image_path0 = 'images/test_images/fcc32dffd24d.png'\n",
    "\n",
    "print(\"The original image is of label 4\")\n",
    "img4 = ef(image_path4)\n",
    "predicted_label4 = model.predict(img4)\n",
    "print(\"Predicted Label:\", predicted_label4)\n",
    "print(\"Predicted Label:\",predicted_label4.argmax())\n",
    "\n",
    "print(\"The original image is of label 3\")\n",
    "img3 = ef(image_path3)\n",
    "predicted_label3 = model.predict(img3)\n",
    "print(\"Predicted Label:\", predicted_label3)\n",
    "print(\"Predicted Label:\",predicted_label3.argmax())\n",
    "\n",
    "print(\"The original image is of label 2\")\n",
    "img2 = ef(image_path2)\n",
    "predicted_label2 = model.predict(img2)\n",
    "print(\"Predicted Label:\", predicted_label2)\n",
    "print(\"Predicted Label:\",predicted_label2.argmax())\n",
    "\n",
    "print(\"The original image is of label 1\")\n",
    "img1 = ef(image_path1)\n",
    "predicted_label1 = model.predict(img1)\n",
    "print(\"Predicted Label:\", predicted_label1)\n",
    "print(\"Predicted Label:\",predicted_label1.argmax())\n",
    "\n",
    "print(\"The original image is of label 0\")\n",
    "img0 = ef(image_path0)\n",
    "predicted_label0 = model.predict(img0)\n",
    "print(\"Predicted Label:\", predicted_label0)\n",
    "print(\"Predicted Label:\",predicted_label0.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf4ec5-c1fa-4ec9-bea2-e354f36b8bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe967b9-8768-4151-a55a-0b81836770b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
